\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage[a4paper, total={6.5in, 10in}]{geometry}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage{import}
\usepackage{chemformula}
\usepackage{xcolor}
\usepackage{float}
\usepackage{tabto}
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[section]{placeins}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{tikz}
\usepackage{tikzscale}
\usepackage{pgfplots}
\usepackage{array}
\usepackage{gensymb}
\usepackage{array}
\usetikzlibrary{shapes, arrows}

% Document Parameters
\newcommand\reportTitle{ENCE464 Assignment 2}
\newcommand\subTitle{Jacobi Relaxation Optimisation}

\newcommand\todo[1]{\textcolor{red}{#1}}


% Figure Setup
\tikzstyle{boxes} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text
width=3cm, draw=black]

\tikzstyle{diamond} = [diamond, minimum width=2cm, minimum height=1cm, text centred, text
width=2cm, draw=black]

\tikzstyle{line} = [thick, -, >=stealth]
\tikzstyle{arrow} = [thick, ->, >=stealth]

\author{\LARGE Josiah Craw (35046080)\vspace{1cm}\\\LARGE Ryan Adams (49739800)\vspace{1cm}\\}
\title{\rule{\textwidth}{0.8pt} \\ {\huge \textbf{\reportTitle}}\\{\large \subTitle} \rule{\textwidth}{0.8pt}}

\begin{document}
% Titlepage
\maketitle
\thispagestyle{empty}
\newpage

% Beginning of the report
\setcounter{page}{1}
\section{Introduction}
Optimisation of algorithms and programs is important for increasing data throughput and
minimising computation time. For an implementation on a modern computer, the large CPU caches and multiple CPU cores
of modern processors are able to be utilised to increase the computational speeds of algorithms. The task for this
assignment was to optimise a Jacobi relaxation algorithm for use on a modern computer system, such that the
computational time to complete this process was minimised. The Jacobi relaxation is an iterative algorithm that 
is used to approximate solutions for various differential equations. For our application, this involved solving the 
discrete form of Poissonâ€™s equation by repeatedly iterating over a 3D array. Program optimisation was achieved through 
the use of multithreading, cache optimisations, and compiler optimisations. Section 2 presents the overall design and 
implementation of these optimisations. The testing architecture is outlined in Section 3, before the results of the 
optimisations are presented. The results are analysed in Section 4, where the effectiveness of each optimisation is 
explored. Finally, conclusions are drawn in Section 5.

\section{Design}

\subsection{For Loop Reordering}
One of the largest problems with the supplied demo algorithm was the non-contiguous access of memory. The nested for
loop order went y, z, x, which is inefficient due to the cache wanting to access memory locations close to
the last memory location accessed. To mitigate this, all nested for loops were rearranged to the order z, y, x.

\subsection{Threading Implementation}
Multithreading was implemented by slicing the 3D array along the z-axis and allocating each thread one slice to operate on.
This slicing of the 3D array can be visualised in Figure \ref{fig:block-design} below, which shows an example of how the 3D array
would be shared amongst four threads. This method of implementation was chosen as each slice of memory is contiguous,
allowing each CPU core cache to access the array row-wise. The spatial locality created by this implementation allows
the cache to quickly access memory for use by its CPU core.

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.2\textwidth]{fig/blocks.png}
        \caption{The slicing of the z-axis to divide the array between multiple threads; four in this case.}
        \label{fig:block-design}
    \end{center}
\end{figure}

One problem that arose was the need to synchronise all threads at the end of each iteration of the Jacobi relaxation.
This was achieved through the use of thread barriers at the end of each iteration. The thread barrier blocks each
thread from continuing until all threads have hit the barrier, at which point the barrier releases all the threads
simultaneously. This synchronises all threads at the end of each iteration.

\section{Results}

\subsection{Testing Architecture}
The system used for the testing and benchmarking of the designed algorithm contained an Intel i7-8700 CPU and
32GB of DDR4 RAM operating at 2666MHz. The Intel i7-8700 contains six CPU cores and 12 CPU threads, which operate
at a base clock speed of 3.2GHz and can boost to 4.3GHz on all six cores. 

This processor utilises a split L1 cache, with each CPU core containing 32 KiB of L1i cache for storing instructions,
and 32 KiB of L1d cache for storing data. This combines to give the hexa-core processor 192 KiB of L1i cache and
192 KiB of L1d cache, for a total of 384 KiB of L1 cache. Additionally, each CPU core has access to 256 KiB of L2
cache, combining for a total of 1.5 MiB of L2 cache. Finally, there are 12 MiB of L3 cache which is shared among
all CPU cores.  All of these caches operate with a write-back write policy. The L1 cache is 8-way associative, the
L2 cache is 4-way associative, and the L3 cache is 16-way associative.

\subsection{Timing Results}
Plots of the results can be found in Figures \ref{fig:speed}, \ref{fig:speed2} and \ref{fig:comp}. As shown in Figure \ref{fig:comp}
a significant improvement occurs after the loops are reordered as this results in significantly fewer cache misses. The subsequent
improvements from loop reordering are relatively minor with threading having the largest improvement.

\begin{figure}
\end{figure}
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \begin{adjustbox}{width=\textwidth}
        \input{fig/st.pgf}
        \end{adjustbox}
        \caption{Computation time against dimension size}
        \label{fig:st}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \begin{adjustbox}{width=\textwidth}
        \input{fig/tt.pgf}
        \end{adjustbox}
        \caption{Number of threads to computation time}
        \label{fig:tt}
    \end{subfigure}
    \caption{Computation comparisons to threads and size, each repeated 10 times and averaged with 100 iterations}
    \label{fig:speed}
\end{figure}


\begin{figure}[H]
    \centering
    \begin{adjustbox}{width=0.6\textwidth}
    \input{fig/stt.pgf}
    \end{adjustbox}
    \caption{Number of threads to computation time}
    \label{fig:speed2}
\end{figure}

\begin{figure}
    \centering
    \begin{adjustbox}{width=0.6\textwidth}
    \input{fig/st-comp.pgf}
    \end{adjustbox}
    \caption{A comparison of the different versions of the program}
    \label{fig:comp}
\end{figure}


\section{Analysis}

\subsection{Cache}
The first improvement to the spacial and temporal locality of the cache was the reordering of the loops from the naive approach.
The original code had loop access in the order x, z, y. This resulted in many cache misses as the memory access is not contiguous.
Reordering this resulted in a large speed improvement with the naive approach taking 18m31.760s for a size of 501 and 100 iterations, and the reordered approach
had a time of 3m16.929s. The Naive approach had far more cache misses when compared to the final solution Table \ref{table:cache}.

\begin{table}[H]
    \centering
    \caption{Cache Misses for each version of the poisson test at a size of 301 with 10 iterations}
    \label{table:cache}
    \begin{tabular}{ | m{4cm} | m{2cm} | m{2cm} | m{2cm} | m{2cm} | }
        \hline
        & Naive & Reordered & Unswitched & Final\\
        \hline
        I1 Miss Rate (\%) & 0.00 & 0.00 & 0.00 & 0.00\\
        \hline
        LLi Miss Rate (\%) & 0.00 & 0.00 & 0.00 & 0.00\\
        \hline
        D1 Miss Rate (\%) & 1.0 & 1.2 & 6.2 & 0.9\\
        \hline
        LLd Miss Rate (\%) & 0.7 & 0.8 & 4.0 & 0.7\\
        \hline
        LL Miss Rate (\%) & 0.2 & 0.3 & 1.3 & 0.3\\
        \hline
        
    \end{tabular}
\end{table}

\subsection{Threading}
The results in Figures \ref{fig:speed} and \ref{fig:speed2} show how the use of multiple threads reduces the computation time of the Jacobi relaxation.
The runtime of the algorithm drops quickly as more threads are added, showing how the algorithm is making good use
of the CPUs multiple cores and processing threads. Interestingly, the runtime of the program continues to decrease
as the number of program threads passes the maximum number of CPU threads; even up to 256 program threads. This may
be because the z-slices are closer together and the program allocates threads from the top down. This would mean
the L3 cache is filled with data from near the start of the 3D array instead of data from further down,
improving contiguous data accesses from memory.

\subsection{Optimisation}
The results in Table \ref{table:opt} show the time taken to perform 100 iterations of an array of size N=501,
utilising 32 threads. The size of the program is also shown, as compiler optimisation levels such as -O3 perform
optimisations such as loop unrolling which can increase the program size. The results clearly show that the
different optimisation levels do not reduce the program runtime by any significant amount; nor do they change
the program size. These results were unexpected, as it was anticipated that increasing the optimisation levels
would reduce the program runtime. Similarly, the program size also did not increase as expected. This could be
because of loop unswitching that was optimised within the programs C source code removing the largest optimisation
for the compiler. This could mean that remaining optimisations have a minimal effect, and could be easily
obscured by runtime variation. 

\begin{table}[H]
    \centering
    \caption{Speed and file size of different compiler optimisations at N=501 with 100 iterations}
    \label{table:opt}
    \begin{tabular}{ | m{4cm} | m{3cm} | m{3cm} | }
        \hline
        \textbf{GCC Flag} & \textbf{Time} & \textbf{Binary Size}\\
        \hline
        None & 26.966 & 61.1KiB\\
        \hline
        \texttt{-O1} & 26.873 & 61.1KiB\\
        \hline
        \texttt{-O2} & 26.871 & 61.1KiB\\
        \hline
        \texttt{-O3} & 26.924 & 61.1KiB\\
        \hline
        \texttt{-Os} & 26.952 & 61.1KiB\\
        \hline
        \texttt{-Og} & 26.878 & 61.1KiB\\
        \hline
        \texttt{-Ofast} & 26.840 & 61.1KiB\\
        \hline
    \end{tabular}
\end{table}

\subsection{Profiling}
The Perf Linux utility was used to profile the different code revisions at a size of 301 with 10 iterations, the tasks with the largest overheads are
listed in Table \ref{table:perf}

\begin{table}[H]
    \centering
    \caption{The biggest tasks for each code version}
    \label{table:perf}
    \begin{tabular}{ | m{2cm} | m{4cm} | m{6cm} | m{3.5cm} | }
        \hline
        Version & Task 1 (Overhead) & Task 2 (Overhead) & Task 3 (Overhead)\\
        \hline
        Final & \texttt{poisson\_mid\_task} (93.16\%) & \texttt{poisson\_top\_task} (3.16\%) & \texttt{poisson\_bot\_task} (3.02\%)\\
        \hline
        Naive & \texttt{poisson\_dirichlet} (99.13\%) & \texttt{\_\_memmove\_avx\_unaligned\_erms} (0.65\%) & -\\
        \hline
        Reorder & \texttt{poisson\_dirichlet} (95.89\%) & \texttt{\_\_memmove\_avx\_unaligned\_erms} (3.56\%) & -\\
        \hline
        Unswitched & \texttt{poisson\_dirichlet} (95.59\%) & \texttt{\_\_memmove\_avx\_unaligned\_erms} (3.84\%) & -\\
        \hline
    \end{tabular}
\end{table}

As shown in Table \ref{table:perf} the Naive approach spends almost all of the time in the \texttt{poisson\_dirichlet} this is
due to almost all of the computation occurring in this function. This number reduces with in the reordered loops and the loop
unswitched versions. The threaded version then has a significant increase in time spent within the computation functions.
This is shown in the significant speed improvement in the threaded implementation.

\section{Conclusion}
The optimisation of a Jacobi relaxation algorithm has taken place in order to minimise the runtime of the algorithm
on a modern computer system. This optimisation process has employed multi-threading and cache optimisations to
minimise the runtime of the algorithm. These two optimisation areas created the biggest decreases in runtime out
of any other optimisations. Surprisingly, compiler optimisations made little difference to both the runtime and
the size of the program.

\newpage
\medskip
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}